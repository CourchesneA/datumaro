{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "652f6b58",
   "metadata": {},
   "source": [
    "# Filter datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca821a19",
   "metadata": {},
   "source": [
    "In this notebook example, we'll take a look at Datumaro filter api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da198c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datumaro as dm\n",
    "from datumaro.components.operations import compute_image_statistics, compute_ann_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2cf885",
   "metadata": {},
   "source": [
    "### Filtered by subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031f1d62",
   "metadata": {},
   "source": [
    "We export sample VOC dataset to filter only train subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9640838",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dm.Dataset.import_from('./tests/assets/voc_dataset/voc_dataset1', format='voc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d38cfc9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics for a sample VOC dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': {'images count': 2,\n",
       "  'unique images count': 1,\n",
       "  'repeated images count': 1,\n",
       "  'repeated images': [[('2007_000001', 'train'), ('2007_000002', 'test')]]},\n",
       " 'subsets': {'test': {'images count': 1,\n",
       "   'image mean': [0.9999999999999971, 0.9999999999999971, 0.9999999999999971],\n",
       "   'image std': [9.411065220006367e-08,\n",
       "    9.411065220006367e-08,\n",
       "    9.411065220006367e-08]},\n",
       "  'train': {'images count': 1,\n",
       "   'image mean': [0.9999999999999971, 0.9999999999999971, 0.9999999999999971],\n",
       "   'image std': [9.411065220006367e-08,\n",
       "    9.411065220006367e-08,\n",
       "    9.411065220006367e-08]}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('statistics for a sample VOC dataset')\n",
    "compute_image_statistics(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0e0346",
   "metadata": {},
   "source": [
    "In VOC dataset, there are 'train' and 'test' subset. We will filter only 'train' subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51bf3388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datumaro.components.dataset.Dataset at 0x7fa155a47610>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.Dataset.filter(dataset, '/item[subset=\"train\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb608396",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics for train subset VOC dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': {'images count': 1,\n",
       "  'unique images count': 1,\n",
       "  'repeated images count': 0,\n",
       "  'repeated images': []},\n",
       " 'subsets': {'train': {'images count': 1,\n",
       "   'image mean': [0.9999999999999971, 0.9999999999999971, 0.9999999999999971],\n",
       "   'image std': [9.411065220006367e-08,\n",
       "    9.411065220006367e-08,\n",
       "    9.411065220006367e-08]}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('statistics for train subset VOC dataset')\n",
    "compute_image_statistics(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef29383",
   "metadata": {},
   "source": [
    "### Filtered by id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3807aff",
   "metadata": {},
   "source": [
    "We export sample widerface dataset to filter only dataset which id is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8993b993",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dm.Dataset.import_from('./tests/assets/widerface_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11362daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics for a sample WiderFace dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': {'images count': 3,\n",
       "  'unique images count': 1,\n",
       "  'repeated images count': 1,\n",
       "  'repeated images': [[('0_Parade_image_01', 'train'),\n",
       "    ('0_Parade_image_03', 'val'),\n",
       "    ('1_Handshaking_image_02', 'train')]]},\n",
       " 'subsets': {'val': {'images count': 1,\n",
       "   'image mean': [0.9999999999999973, 0.9999999999999973, 0.9999999999999973],\n",
       "   'image std': [9.058862863930295e-08,\n",
       "    9.058862863930295e-08,\n",
       "    9.058862863930295e-08]},\n",
       "  'train': {'images count': 2,\n",
       "   'image mean': [0.9999999999999973, 0.9999999999999973, 0.9999999999999973],\n",
       "   'image std': [9.043701576544718e-08,\n",
       "    9.043701576544718e-08,\n",
       "    9.043701576544718e-08]}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('statistics for a sample WiderFace dataset')\n",
    "compute_image_statistics(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ed4a847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datumaro.components.dataset.Dataset at 0x7fa154735bd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.Dataset.filter(dataset, '/item[id=\"0_Parade_image_01\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d689f5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics for WiderFace dataset id == 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': {'images count': 1,\n",
       "  'unique images count': 1,\n",
       "  'repeated images count': 0,\n",
       "  'repeated images': []},\n",
       " 'subsets': {'train': {'images count': 1,\n",
       "   'image mean': [0.9999999999999973, 0.9999999999999973, 0.9999999999999973],\n",
       "   'image std': [9.058862863930295e-08,\n",
       "    9.058862863930295e-08,\n",
       "    9.058862863930295e-08]}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('statistics for WiderFace dataset id == 1')\n",
    "compute_image_statistics(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b23d1af",
   "metadata": {},
   "source": [
    "### Filtered by width and height"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ca87d",
   "metadata": {},
   "source": [
    "We export sample dataset to extract a dataset with images with width < height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5e30736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Not implemented: Found potentially conflicting source types with labels: person_keypoints, labels, instances, panoptic, stuff. Only one type will be used: instances\n",
      "WARNING:root:Not implemented: conflicting source './tests/assets/coco_dataset/coco/annotations/labels_train.json' is skipped.\n",
      "WARNING:root:Not implemented: conflicting source './tests/assets/coco_dataset/coco/annotations/person_keypoints_train.json' is skipped.\n",
      "WARNING:root:Not implemented: conflicting source './tests/assets/coco_dataset/coco/annotations/stuff_train.json' is skipped.\n",
      "WARNING:root:Not implemented: conflicting source './tests/assets/coco_dataset/coco/annotations/panoptic_train.json' is skipped.\n",
      "WARNING:root:Not implemented: conflicting source './tests/assets/coco_dataset/coco/annotations/panoptic_val.json' is skipped.\n",
      "WARNING:root:Not implemented: conflicting source './tests/assets/coco_dataset/coco/annotations/labels_val.json' is skipped.\n",
      "WARNING:root:Not implemented: conflicting source './tests/assets/coco_dataset/coco/annotations/person_keypoints_val.json' is skipped.\n",
      "WARNING:root:Not implemented: conflicting source './tests/assets/coco_dataset/coco/annotations/stuff_val.json' is skipped.\n"
     ]
    }
   ],
   "source": [
    "dataset = dm.Dataset.import_from('./tests/assets/coco_dataset/coco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f8953e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics for a sample coco dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': {'images count': 2,\n",
       "  'unique images count': 1,\n",
       "  'repeated images count': 1,\n",
       "  'repeated images': [[('a', 'train'), ('b', 'val')]]},\n",
       " 'subsets': {'train': {'images count': 1,\n",
       "   'image mean': [0.9999999999999987, 0.9999999999999987, 0.9999999999999987],\n",
       "   'image std': [6.361265799828938e-08,\n",
       "    6.361265799828938e-08,\n",
       "    6.361265799828938e-08]},\n",
       "  'val': {'images count': 1,\n",
       "   'image mean': [0.9999999999999987, 0.9999999999999987, 0.9999999999999987],\n",
       "   'image std': [6.361265799828938e-08,\n",
       "    6.361265799828938e-08,\n",
       "    6.361265799828938e-08]}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('statistics for a sample coco dataset')\n",
    "compute_image_statistics(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b693479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datumaro.components.dataset.Dataset at 0x7fa194a1a250>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.Dataset.filter(dataset, '/item[image/width < image/height]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7479f9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics for width < height coco dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': {'images count': 1,\n",
       "  'unique images count': 1,\n",
       "  'repeated images count': 0,\n",
       "  'repeated images': []},\n",
       " 'subsets': {'val': {'images count': 1,\n",
       "   'image mean': [0.9999999999999987, 0.9999999999999987, 0.9999999999999987],\n",
       "   'image std': [6.361265799828938e-08,\n",
       "    6.361265799828938e-08,\n",
       "    6.361265799828938e-08]}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('statistics for width < height coco dataset')\n",
    "compute_image_statistics(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbde6b17",
   "metadata": {},
   "source": [
    "### Filtered by label and area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0dc1de",
   "metadata": {},
   "source": [
    "We export sample dataset to extract only non-`persons`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c8248a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dm.Dataset.import_from('./tests/assets/voc_dataset/voc_dataset1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98ad83ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation count for coco dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('annotation count for voc dataset')\n",
    "compute_ann_statistics(dataset)['annotations count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d74e4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': {'count': 15,\n",
       "  'distribution': {'background': [0, 0.0],\n",
       "   'aeroplane': [1, 0.06666666666666667],\n",
       "   'bicycle': [1, 0.06666666666666667],\n",
       "   'bird': [1, 0.06666666666666667],\n",
       "   'boat': [0, 0.0],\n",
       "   'bottle': [1, 0.06666666666666667],\n",
       "   'bus': [0, 0.0],\n",
       "   'car': [1, 0.06666666666666667],\n",
       "   'cat': [1, 0.06666666666666667],\n",
       "   'chair': [1, 0.06666666666666667],\n",
       "   'cow': [0, 0.0],\n",
       "   'diningtable': [1, 0.06666666666666667],\n",
       "   'dog': [0, 0.0],\n",
       "   'horse': [1, 0.06666666666666667],\n",
       "   'motorbike': [0, 0.0],\n",
       "   'person': [2, 0.13333333333333333],\n",
       "   'pottedplant': [0, 0.0],\n",
       "   'sheep': [1, 0.06666666666666667],\n",
       "   'sofa': [0, 0.0],\n",
       "   'train': [1, 0.06666666666666667],\n",
       "   'tvmonitor': [0, 0.0],\n",
       "   'ignored': [1, 0.06666666666666667],\n",
       "   'head': [1, 0.06666666666666667],\n",
       "   'hand': [0, 0.0],\n",
       "   'foot': [0, 0.0]},\n",
       "  'attributes': {'difficult': {'count': 2,\n",
       "    'values count': 1,\n",
       "    'values present': ['False'],\n",
       "    'distribution': {'False': [2, 1.0]}},\n",
       "   'truncated': {'count': 2,\n",
       "    'values count': 2,\n",
       "    'values present': ['False', 'True'],\n",
       "    'distribution': {'False': [1, 0.5], 'True': [1, 0.5]}},\n",
       "   'jumping': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['False'],\n",
       "    'distribution': {'False': [1, 1.0]}},\n",
       "   'playinginstrument': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['False'],\n",
       "    'distribution': {'False': [1, 1.0]}},\n",
       "   'reading': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['True'],\n",
       "    'distribution': {'True': [1, 1.0]}},\n",
       "   'ridingbike': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['False'],\n",
       "    'distribution': {'False': [1, 1.0]}},\n",
       "   'takingphoto': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['True'],\n",
       "    'distribution': {'True': [1, 1.0]}},\n",
       "   'usingcomputer': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['False'],\n",
       "    'distribution': {'False': [1, 1.0]}},\n",
       "   'walking': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['True'],\n",
       "    'distribution': {'True': [1, 1.0]}},\n",
       "   'running': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['False'],\n",
       "    'distribution': {'False': [1, 1.0]}},\n",
       "   'other': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['True'],\n",
       "    'distribution': {'True': [1, 1.0]}},\n",
       "   'phoning': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['True'],\n",
       "    'distribution': {'True': [1, 1.0]}},\n",
       "   'ridinghorse': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['True'],\n",
       "    'distribution': {'True': [1, 1.0]}},\n",
       "   'pose': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['Unspecified'],\n",
       "    'distribution': {'Unspecified': [1, 1.0]}}}},\n",
       " 'segments': {'avg. area': 53.0,\n",
       "  'area distribution': [{'min': 4.0, 'max': 23.6, 'count': 3, 'percent': 0.75},\n",
       "   {'min': 23.6, 'max': 43.2, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 43.2, 'max': 62.800000000000004, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 62.800000000000004, 'max': 82.4, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 82.4, 'max': 102.0, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 102.0, 'max': 121.60000000000001, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 121.60000000000001,\n",
       "    'max': 141.20000000000002,\n",
       "    'count': 0,\n",
       "    'percent': 0.0},\n",
       "   {'min': 141.20000000000002, 'max': 160.8, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 160.8, 'max': 180.4, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 180.4, 'max': 200.0, 'count': 1, 'percent': 0.25}],\n",
       "  'pixel distribution': {'background': [0, 0.0],\n",
       "   'aeroplane': [0, 0.0],\n",
       "   'bicycle': [200, 0.9433962264150944],\n",
       "   'bird': [0, 0.0],\n",
       "   'boat': [0, 0.0],\n",
       "   'bottle': [0, 0.0],\n",
       "   'bus': [0, 0.0],\n",
       "   'car': [0, 0.0],\n",
       "   'cat': [4, 0.018867924528301886],\n",
       "   'chair': [0, 0.0],\n",
       "   'cow': [0, 0.0],\n",
       "   'diningtable': [0, 0.0],\n",
       "   'dog': [0, 0.0],\n",
       "   'horse': [0, 0.0],\n",
       "   'motorbike': [0, 0.0],\n",
       "   'person': [4, 0.018867924528301886],\n",
       "   'pottedplant': [0, 0.0],\n",
       "   'sheep': [0, 0.0],\n",
       "   'sofa': [0, 0.0],\n",
       "   'train': [0, 0.0],\n",
       "   'tvmonitor': [0, 0.0],\n",
       "   'ignored': [0, 0.0],\n",
       "   'head': [4, 0.018867924528301886],\n",
       "   'hand': [0, 0.0],\n",
       "   'foot': [0, 0.0]}}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('annotation statistics for voc dataset whose annotation is label!=\"person\"')\n",
    "compute_ann_statistics(dataset)['annotations']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c88c99",
   "metadata": {},
   "source": [
    "Indicate `filter_annotations` as `True` if filter needs to apply to annotations. The default value is `False` to items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8dda4168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datumaro.components.dataset.Dataset at 0x7fa1bc76b190>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.Dataset.filter(dataset, '/item/annotation[label!=\"person\"]', filter_annotations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67f0b7e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation count for coco dataset whose annotation is (label=\"cat\" and area < 9643950) or label!=\"person\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('annotation count for voc dataset whose annotation is label!=\"person\"')\n",
    "compute_ann_statistics(dataset)['annotations count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "506ae22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': {'count': 13,\n",
       "  'distribution': {'background': [0, 0.0],\n",
       "   'aeroplane': [1, 0.07692307692307693],\n",
       "   'bicycle': [1, 0.07692307692307693],\n",
       "   'bird': [1, 0.07692307692307693],\n",
       "   'boat': [0, 0.0],\n",
       "   'bottle': [1, 0.07692307692307693],\n",
       "   'bus': [0, 0.0],\n",
       "   'car': [1, 0.07692307692307693],\n",
       "   'cat': [1, 0.07692307692307693],\n",
       "   'chair': [1, 0.07692307692307693],\n",
       "   'cow': [0, 0.0],\n",
       "   'diningtable': [1, 0.07692307692307693],\n",
       "   'dog': [0, 0.0],\n",
       "   'horse': [1, 0.07692307692307693],\n",
       "   'motorbike': [0, 0.0],\n",
       "   'person': [0, 0.0],\n",
       "   'pottedplant': [0, 0.0],\n",
       "   'sheep': [1, 0.07692307692307693],\n",
       "   'sofa': [0, 0.0],\n",
       "   'train': [1, 0.07692307692307693],\n",
       "   'tvmonitor': [0, 0.0],\n",
       "   'ignored': [1, 0.07692307692307693],\n",
       "   'head': [1, 0.07692307692307693],\n",
       "   'hand': [0, 0.0],\n",
       "   'foot': [0, 0.0]},\n",
       "  'attributes': {'difficult': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['False'],\n",
       "    'distribution': {'False': [1, 1.0]}},\n",
       "   'truncated': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['True'],\n",
       "    'distribution': {'True': [1, 1.0]}},\n",
       "   'pose': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['Unspecified'],\n",
       "    'distribution': {'Unspecified': [1, 1.0]}}}},\n",
       " 'segments': {'avg. area': 69.33333333333333,\n",
       "  'area distribution': [{'min': 4.0,\n",
       "    'max': 23.6,\n",
       "    'count': 2,\n",
       "    'percent': 0.6666666666666666},\n",
       "   {'min': 23.6, 'max': 43.2, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 43.2, 'max': 62.800000000000004, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 62.800000000000004, 'max': 82.4, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 82.4, 'max': 102.0, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 102.0, 'max': 121.60000000000001, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 121.60000000000001,\n",
       "    'max': 141.20000000000002,\n",
       "    'count': 0,\n",
       "    'percent': 0.0},\n",
       "   {'min': 141.20000000000002, 'max': 160.8, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 160.8, 'max': 180.4, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 180.4, 'max': 200.0, 'count': 1, 'percent': 0.3333333333333333}],\n",
       "  'pixel distribution': {'background': [0, 0.0],\n",
       "   'aeroplane': [0, 0.0],\n",
       "   'bicycle': [200, 0.9615384615384616],\n",
       "   'bird': [0, 0.0],\n",
       "   'boat': [0, 0.0],\n",
       "   'bottle': [0, 0.0],\n",
       "   'bus': [0, 0.0],\n",
       "   'car': [0, 0.0],\n",
       "   'cat': [4, 0.019230769230769232],\n",
       "   'chair': [0, 0.0],\n",
       "   'cow': [0, 0.0],\n",
       "   'diningtable': [0, 0.0],\n",
       "   'dog': [0, 0.0],\n",
       "   'horse': [0, 0.0],\n",
       "   'motorbike': [0, 0.0],\n",
       "   'person': [0, 0.0],\n",
       "   'pottedplant': [0, 0.0],\n",
       "   'sheep': [0, 0.0],\n",
       "   'sofa': [0, 0.0],\n",
       "   'train': [0, 0.0],\n",
       "   'tvmonitor': [0, 0.0],\n",
       "   'ignored': [0, 0.0],\n",
       "   'head': [4, 0.019230769230769232],\n",
       "   'hand': [0, 0.0],\n",
       "   'foot': [0, 0.0]}}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_ann_statistics(dataset)['annotations']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3201198f",
   "metadata": {},
   "source": [
    "### Filtered by annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1e403a",
   "metadata": {},
   "source": [
    "We export sample dataset to extract non-occluded annotations, remove empty images. Use data only from the “s1” source of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "732face4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dm.Dataset.import_from('./tests/assets/voc_dataset/voc_dataset1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7badb290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image statistics for sample voc dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': {'images count': 2,\n",
       "  'unique images count': 1,\n",
       "  'repeated images count': 1,\n",
       "  'repeated images': [[('2007_000001', 'train'), ('2007_000002', 'test')]]},\n",
       " 'subsets': {'test': {'images count': 1,\n",
       "   'image mean': [0.9999999999999971, 0.9999999999999971, 0.9999999999999971],\n",
       "   'image std': [9.411065220006367e-08,\n",
       "    9.411065220006367e-08,\n",
       "    9.411065220006367e-08]},\n",
       "  'train': {'images count': 1,\n",
       "   'image mean': [0.9999999999999971, 0.9999999999999971, 0.9999999999999971],\n",
       "   'image std': [9.411065220006367e-08,\n",
       "    9.411065220006367e-08,\n",
       "    9.411065220006367e-08]}}}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('image statistics for sample voc dataset')\n",
    "compute_image_statistics(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "326af8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation statistics for sample voc dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('annotation statistics for sample voc dataset')\n",
    "compute_ann_statistics(dataset)['annotations count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f8ddf924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datumaro.components.dataset.Dataset at 0x7efc3ba511d0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.Dataset.filter(dataset, '/item/annotation[occluded=\"False\"]', filter_annotations=True, remove_empty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3a971546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image statistics for non-occluded annotations and empty images removed voc dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': {'images count': 1,\n",
       "  'unique images count': 1,\n",
       "  'repeated images count': 0,\n",
       "  'repeated images': []},\n",
       " 'subsets': {'train': {'images count': 1,\n",
       "   'image mean': [0.9999999999999971, 0.9999999999999971, 0.9999999999999971],\n",
       "   'image std': [9.411065220006367e-08,\n",
       "    9.411065220006367e-08,\n",
       "    9.411065220006367e-08]}}}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('image statistics for non-occluded annotations and empty images removed voc dataset')\n",
    "compute_image_statistics(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f66b97ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation statistics for non-occluded annotations and empty images removed voc dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('annotation statistics for non-occluded annotations and empty images removed voc dataset')\n",
    "compute_ann_statistics(dataset)['annotations count']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
