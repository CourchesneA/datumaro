{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "652f6b58",
   "metadata": {},
   "source": [
    "# Filter datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca821a19",
   "metadata": {},
   "source": [
    "In this notebook example, we'll take a look at Datumaro filter api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da198c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (C) 2022 Intel Corporation\n",
    "#\n",
    "# SPDX-License-Identifier: MIT\n",
    "\n",
    "import os\n",
    "import datumaro as dm\n",
    "from datumaro.components.operations import compute_image_statistics, compute_ann_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2cf885",
   "metadata": {},
   "source": [
    "### Filtered by subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031f1d62",
   "metadata": {},
   "source": [
    "We export sample VOC dataset to filter only train subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9640838",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dm.Dataset.import_from('./tests/assets/voc_dataset/voc_dataset1', format='voc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d38cfc9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics for a sample VOC dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': {'images count': 2,\n",
       "  'unique images count': 1,\n",
       "  'repeated images count': 1,\n",
       "  'repeated images': [[('2007_000001', 'train'), ('2007_000002', 'test')]]},\n",
       " 'subsets': {'test': {'images count': 1,\n",
       "   'image mean': [0.9999999999999971, 0.9999999999999971, 0.9999999999999971],\n",
       "   'image std': [9.411065220006367e-08,\n",
       "    9.411065220006367e-08,\n",
       "    9.411065220006367e-08]},\n",
       "  'train': {'images count': 1,\n",
       "   'image mean': [0.9999999999999971, 0.9999999999999971, 0.9999999999999971],\n",
       "   'image std': [9.411065220006367e-08,\n",
       "    9.411065220006367e-08,\n",
       "    9.411065220006367e-08]}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('statistics for a sample VOC dataset')\n",
    "compute_image_statistics(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0e0346",
   "metadata": {},
   "source": [
    "In VOC dataset, there are 'train' and 'test' subset. We will filter only 'train' subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51bf3388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datumaro.components.dataset.Dataset at 0x7f864f425a90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.Dataset.filter(dataset, '/item[subset=\"train\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb608396",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics for train subset VOC dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': {'images count': 1,\n",
       "  'unique images count': 1,\n",
       "  'repeated images count': 0,\n",
       "  'repeated images': []},\n",
       " 'subsets': {'train': {'images count': 1,\n",
       "   'image mean': [0.9999999999999971, 0.9999999999999971, 0.9999999999999971],\n",
       "   'image std': [9.411065220006367e-08,\n",
       "    9.411065220006367e-08,\n",
       "    9.411065220006367e-08]}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('statistics for train subset VOC dataset')\n",
    "compute_image_statistics(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef29383",
   "metadata": {},
   "source": [
    "### Filtered by id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3807aff",
   "metadata": {},
   "source": [
    "We export sample widerface dataset to filter only dataset which id is `id=0_Parade_image_01`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8993b993",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dm.Dataset.import_from('./tests/assets/widerface_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11362daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics for a sample WiderFace dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': {'images count': 3,\n",
       "  'unique images count': 1,\n",
       "  'repeated images count': 1,\n",
       "  'repeated images': [[('0_Parade_image_01', 'train'),\n",
       "    ('0_Parade_image_03', 'val'),\n",
       "    ('1_Handshaking_image_02', 'train')]]},\n",
       " 'subsets': {'val': {'images count': 1,\n",
       "   'image mean': [0.9999999999999973, 0.9999999999999973, 0.9999999999999973],\n",
       "   'image std': [9.058862863930295e-08,\n",
       "    9.058862863930295e-08,\n",
       "    9.058862863930295e-08]},\n",
       "  'train': {'images count': 2,\n",
       "   'image mean': [0.9999999999999973, 0.9999999999999973, 0.9999999999999973],\n",
       "   'image std': [9.043701576544718e-08,\n",
       "    9.043701576544718e-08,\n",
       "    9.043701576544718e-08]}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('statistics for a sample WiderFace dataset')\n",
    "compute_image_statistics(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ed4a847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datumaro.components.dataset.Dataset at 0x7f864f3de850>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.Dataset.filter(dataset, '/item[id=\"0_Parade_image_01\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d689f5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics for WiderFace dataset id == 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': {'images count': 1,\n",
       "  'unique images count': 1,\n",
       "  'repeated images count': 0,\n",
       "  'repeated images': []},\n",
       " 'subsets': {'train': {'images count': 1,\n",
       "   'image mean': [0.9999999999999973, 0.9999999999999973, 0.9999999999999973],\n",
       "   'image std': [9.058862863930295e-08,\n",
       "    9.058862863930295e-08,\n",
       "    9.058862863930295e-08]}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('statistics for WiderFace dataset id == 1')\n",
    "compute_image_statistics(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b23d1af",
   "metadata": {},
   "source": [
    "### Filtered by width and height"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ca87d",
   "metadata": {},
   "source": [
    "We export sample dataset to extract a dataset with images with width < height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5e30736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Not implemented: Found potentially conflicting source types with labels: labels, instances, person_keypoints, stuff, panoptic. Only one type will be used: instances\n",
      "WARNING:root:Not implemented: conflicting source './tests/assets/coco_dataset/coco/annotations/labels_train.json' is skipped.\n",
      "WARNING:root:Not implemented: conflicting source './tests/assets/coco_dataset/coco/annotations/person_keypoints_train.json' is skipped.\n",
      "WARNING:root:Not implemented: conflicting source './tests/assets/coco_dataset/coco/annotations/stuff_train.json' is skipped.\n",
      "WARNING:root:Not implemented: conflicting source './tests/assets/coco_dataset/coco/annotations/panoptic_train.json' is skipped.\n",
      "WARNING:root:Not implemented: conflicting source './tests/assets/coco_dataset/coco/annotations/panoptic_val.json' is skipped.\n",
      "WARNING:root:Not implemented: conflicting source './tests/assets/coco_dataset/coco/annotations/labels_val.json' is skipped.\n",
      "WARNING:root:Not implemented: conflicting source './tests/assets/coco_dataset/coco/annotations/person_keypoints_val.json' is skipped.\n",
      "WARNING:root:Not implemented: conflicting source './tests/assets/coco_dataset/coco/annotations/stuff_val.json' is skipped.\n"
     ]
    }
   ],
   "source": [
    "dataset = dm.Dataset.import_from('./tests/assets/coco_dataset/coco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87feaf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_width_height(dataset: dm.Dataset):\n",
    "    size_dict = {}\n",
    "    for item in dataset:\n",
    "        size_dict[item.id] = item.media.size\n",
    "    return size_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b821387a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width and height for a sample coco dataset images\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a': (5, 10), 'b': (10, 5)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('width and height for a sample coco dataset images')\n",
    "get_width_height(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b693479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datumaro.components.dataset.Dataset at 0x7fdae0b713d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.Dataset.filter(dataset, '/item[image/width < image/height]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7479f9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width and height for width < height coco dataset images\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'b': (10, 5)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('width and height for width < height coco dataset images')\n",
    "get_width_height(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbde6b17",
   "metadata": {},
   "source": [
    "### Filtered by label and area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0dc1de",
   "metadata": {},
   "source": [
    "We export sample dataset to extract only non-`persons`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c8248a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dm.Dataset.import_from('./tests/assets/voc_dataset/voc_dataset1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98ad83ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation count for voc dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('annotation count for voc dataset')\n",
    "compute_ann_statistics(dataset)['annotations count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d74e4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation statistics for voc dataset whose annotation is label!=\"person\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'labels': {'count': 15,\n",
       "  'distribution': {'background': [0, 0.0],\n",
       "   'aeroplane': [1, 0.06666666666666667],\n",
       "   'bicycle': [1, 0.06666666666666667],\n",
       "   'bird': [1, 0.06666666666666667],\n",
       "   'boat': [0, 0.0],\n",
       "   'bottle': [1, 0.06666666666666667],\n",
       "   'bus': [0, 0.0],\n",
       "   'car': [1, 0.06666666666666667],\n",
       "   'cat': [1, 0.06666666666666667],\n",
       "   'chair': [1, 0.06666666666666667],\n",
       "   'cow': [0, 0.0],\n",
       "   'diningtable': [1, 0.06666666666666667],\n",
       "   'dog': [0, 0.0],\n",
       "   'horse': [1, 0.06666666666666667],\n",
       "   'motorbike': [0, 0.0],\n",
       "   'person': [2, 0.13333333333333333],\n",
       "   'pottedplant': [0, 0.0],\n",
       "   'sheep': [1, 0.06666666666666667],\n",
       "   'sofa': [0, 0.0],\n",
       "   'train': [1, 0.06666666666666667],\n",
       "   'tvmonitor': [0, 0.0],\n",
       "   'ignored': [1, 0.06666666666666667],\n",
       "   'head': [1, 0.06666666666666667],\n",
       "   'hand': [0, 0.0],\n",
       "   'foot': [0, 0.0]},\n",
       "  'attributes': {'difficult': {'count': 2,\n",
       "    'values count': 1,\n",
       "    'values present': ['False'],\n",
       "    'distribution': {'False': [2, 1.0]}},\n",
       "   'truncated': {'count': 2,\n",
       "    'values count': 2,\n",
       "    'values present': ['False', 'True'],\n",
       "    'distribution': {'False': [1, 0.5], 'True': [1, 0.5]}},\n",
       "   'ridinghorse': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['True'],\n",
       "    'distribution': {'True': [1, 1.0]}},\n",
       "   'usingcomputer': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['False'],\n",
       "    'distribution': {'False': [1, 1.0]}},\n",
       "   'other': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['True'],\n",
       "    'distribution': {'True': [1, 1.0]}},\n",
       "   'jumping': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['False'],\n",
       "    'distribution': {'False': [1, 1.0]}},\n",
       "   'phoning': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['True'],\n",
       "    'distribution': {'True': [1, 1.0]}},\n",
       "   'takingphoto': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['True'],\n",
       "    'distribution': {'True': [1, 1.0]}},\n",
       "   'running': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['False'],\n",
       "    'distribution': {'False': [1, 1.0]}},\n",
       "   'walking': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['True'],\n",
       "    'distribution': {'True': [1, 1.0]}},\n",
       "   'reading': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['True'],\n",
       "    'distribution': {'True': [1, 1.0]}},\n",
       "   'playinginstrument': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['False'],\n",
       "    'distribution': {'False': [1, 1.0]}},\n",
       "   'ridingbike': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['False'],\n",
       "    'distribution': {'False': [1, 1.0]}},\n",
       "   'pose': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['Unspecified'],\n",
       "    'distribution': {'Unspecified': [1, 1.0]}}}},\n",
       " 'segments': {'avg. area': 53.0,\n",
       "  'area distribution': [{'min': 4.0, 'max': 23.6, 'count': 3, 'percent': 0.75},\n",
       "   {'min': 23.6, 'max': 43.2, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 43.2, 'max': 62.800000000000004, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 62.800000000000004, 'max': 82.4, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 82.4, 'max': 102.0, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 102.0, 'max': 121.60000000000001, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 121.60000000000001,\n",
       "    'max': 141.20000000000002,\n",
       "    'count': 0,\n",
       "    'percent': 0.0},\n",
       "   {'min': 141.20000000000002, 'max': 160.8, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 160.8, 'max': 180.4, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 180.4, 'max': 200.0, 'count': 1, 'percent': 0.25}],\n",
       "  'pixel distribution': {'background': [0, 0.0],\n",
       "   'aeroplane': [0, 0.0],\n",
       "   'bicycle': [200, 0.9433962264150944],\n",
       "   'bird': [0, 0.0],\n",
       "   'boat': [0, 0.0],\n",
       "   'bottle': [0, 0.0],\n",
       "   'bus': [0, 0.0],\n",
       "   'car': [0, 0.0],\n",
       "   'cat': [4, 0.018867924528301886],\n",
       "   'chair': [0, 0.0],\n",
       "   'cow': [0, 0.0],\n",
       "   'diningtable': [0, 0.0],\n",
       "   'dog': [0, 0.0],\n",
       "   'horse': [0, 0.0],\n",
       "   'motorbike': [0, 0.0],\n",
       "   'person': [4, 0.018867924528301886],\n",
       "   'pottedplant': [0, 0.0],\n",
       "   'sheep': [0, 0.0],\n",
       "   'sofa': [0, 0.0],\n",
       "   'train': [0, 0.0],\n",
       "   'tvmonitor': [0, 0.0],\n",
       "   'ignored': [0, 0.0],\n",
       "   'head': [4, 0.018867924528301886],\n",
       "   'hand': [0, 0.0],\n",
       "   'foot': [0, 0.0]}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('annotation statistics for voc dataset whose annotation is label!=\"person\"')\n",
    "compute_ann_statistics(dataset)['annotations']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c88c99",
   "metadata": {},
   "source": [
    "Indicate `filter_annotations` as `True` if filter needs to apply to annotations. The default value is `False` to items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dda4168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datumaro.components.dataset.Dataset at 0x7f864f3a5e10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.Dataset.filter(dataset, '/item/annotation[label!=\"person\"]', filter_annotations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67f0b7e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation count for voc dataset whose annotation is label!=\"person\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('annotation count for voc dataset whose annotation is label!=\"person\"')\n",
    "compute_ann_statistics(dataset)['annotations count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "506ae22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': {'count': 13,\n",
       "  'distribution': {'background': [0, 0.0],\n",
       "   'aeroplane': [1, 0.07692307692307693],\n",
       "   'bicycle': [1, 0.07692307692307693],\n",
       "   'bird': [1, 0.07692307692307693],\n",
       "   'boat': [0, 0.0],\n",
       "   'bottle': [1, 0.07692307692307693],\n",
       "   'bus': [0, 0.0],\n",
       "   'car': [1, 0.07692307692307693],\n",
       "   'cat': [1, 0.07692307692307693],\n",
       "   'chair': [1, 0.07692307692307693],\n",
       "   'cow': [0, 0.0],\n",
       "   'diningtable': [1, 0.07692307692307693],\n",
       "   'dog': [0, 0.0],\n",
       "   'horse': [1, 0.07692307692307693],\n",
       "   'motorbike': [0, 0.0],\n",
       "   'person': [0, 0.0],\n",
       "   'pottedplant': [0, 0.0],\n",
       "   'sheep': [1, 0.07692307692307693],\n",
       "   'sofa': [0, 0.0],\n",
       "   'train': [1, 0.07692307692307693],\n",
       "   'tvmonitor': [0, 0.0],\n",
       "   'ignored': [1, 0.07692307692307693],\n",
       "   'head': [1, 0.07692307692307693],\n",
       "   'hand': [0, 0.0],\n",
       "   'foot': [0, 0.0]},\n",
       "  'attributes': {'difficult': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['False'],\n",
       "    'distribution': {'False': [1, 1.0]}},\n",
       "   'truncated': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['True'],\n",
       "    'distribution': {'True': [1, 1.0]}},\n",
       "   'pose': {'count': 1,\n",
       "    'values count': 1,\n",
       "    'values present': ['Unspecified'],\n",
       "    'distribution': {'Unspecified': [1, 1.0]}}}},\n",
       " 'segments': {'avg. area': 69.33333333333333,\n",
       "  'area distribution': [{'min': 4.0,\n",
       "    'max': 23.6,\n",
       "    'count': 2,\n",
       "    'percent': 0.6666666666666666},\n",
       "   {'min': 23.6, 'max': 43.2, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 43.2, 'max': 62.800000000000004, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 62.800000000000004, 'max': 82.4, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 82.4, 'max': 102.0, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 102.0, 'max': 121.60000000000001, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 121.60000000000001,\n",
       "    'max': 141.20000000000002,\n",
       "    'count': 0,\n",
       "    'percent': 0.0},\n",
       "   {'min': 141.20000000000002, 'max': 160.8, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 160.8, 'max': 180.4, 'count': 0, 'percent': 0.0},\n",
       "   {'min': 180.4, 'max': 200.0, 'count': 1, 'percent': 0.3333333333333333}],\n",
       "  'pixel distribution': {'background': [0, 0.0],\n",
       "   'aeroplane': [0, 0.0],\n",
       "   'bicycle': [200, 0.9615384615384616],\n",
       "   'bird': [0, 0.0],\n",
       "   'boat': [0, 0.0],\n",
       "   'bottle': [0, 0.0],\n",
       "   'bus': [0, 0.0],\n",
       "   'car': [0, 0.0],\n",
       "   'cat': [4, 0.019230769230769232],\n",
       "   'chair': [0, 0.0],\n",
       "   'cow': [0, 0.0],\n",
       "   'diningtable': [0, 0.0],\n",
       "   'dog': [0, 0.0],\n",
       "   'horse': [0, 0.0],\n",
       "   'motorbike': [0, 0.0],\n",
       "   'person': [0, 0.0],\n",
       "   'pottedplant': [0, 0.0],\n",
       "   'sheep': [0, 0.0],\n",
       "   'sofa': [0, 0.0],\n",
       "   'train': [0, 0.0],\n",
       "   'tvmonitor': [0, 0.0],\n",
       "   'ignored': [0, 0.0],\n",
       "   'head': [4, 0.019230769230769232],\n",
       "   'hand': [0, 0.0],\n",
       "   'foot': [0, 0.0]}}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_ann_statistics(dataset)['annotations']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3201198f",
   "metadata": {},
   "source": [
    "### Filtered by annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1e403a",
   "metadata": {},
   "source": [
    "We export sample dataset to extract non-occluded annotations, remove empty images. Use data only from the “s1” source of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "732face4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dm.Dataset.import_from('./tests/assets/voc_dataset/voc_dataset1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7badb290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image statistics for sample voc dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': {'images count': 2,\n",
       "  'unique images count': 1,\n",
       "  'repeated images count': 1,\n",
       "  'repeated images': [[('2007_000001', 'train'), ('2007_000002', 'test')]]},\n",
       " 'subsets': {'test': {'images count': 1,\n",
       "   'image mean': [0.9999999999999971, 0.9999999999999971, 0.9999999999999971],\n",
       "   'image std': [9.411065220006367e-08,\n",
       "    9.411065220006367e-08,\n",
       "    9.411065220006367e-08]},\n",
       "  'train': {'images count': 1,\n",
       "   'image mean': [0.9999999999999971, 0.9999999999999971, 0.9999999999999971],\n",
       "   'image std': [9.411065220006367e-08,\n",
       "    9.411065220006367e-08,\n",
       "    9.411065220006367e-08]}}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('image statistics for sample voc dataset')\n",
    "compute_image_statistics(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "326af8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation statistics for sample voc dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('annotation statistics for sample voc dataset')\n",
    "compute_ann_statistics(dataset)['annotations count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8ddf924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datumaro.components.dataset.Dataset at 0x7f868b978f90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.Dataset.filter(dataset, '/item/annotation[occluded=\"False\"]', filter_annotations=True, remove_empty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a971546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image statistics for non-occluded annotations and empty images removed voc dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': {'images count': 1,\n",
       "  'unique images count': 1,\n",
       "  'repeated images count': 0,\n",
       "  'repeated images': []},\n",
       " 'subsets': {'train': {'images count': 1,\n",
       "   'image mean': [0.9999999999999971, 0.9999999999999971, 0.9999999999999971],\n",
       "   'image std': [9.411065220006367e-08,\n",
       "    9.411065220006367e-08,\n",
       "    9.411065220006367e-08]}}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('image statistics for non-occluded annotations and empty images removed voc dataset')\n",
    "compute_image_statistics(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f66b97ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation statistics for non-occluded annotations and empty images removed voc dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('annotation statistics for non-occluded annotations and empty images removed voc dataset')\n",
    "compute_ann_statistics(dataset)['annotations count']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
